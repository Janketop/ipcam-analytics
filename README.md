# Аналитика IP-камер (Телефон/Не работает) — Starter (RU)

Готовый стартовый проект: FastAPI (Python), PostgreSQL, Next.js (React).
- Детекция человека/телефона и позы (YOLOv8 + pose)
- События `PHONE_USAGE` и снимки с размытием лица
- MJPEG live-поток с наложенной разметкой (рамки, скелет)
- Дашборд на русском, статистика за 24 часа
- Для каждой камеры настраиваются флаги: детекция телефонов, автомобилей и сохранение времени въезда

## Запуск
```
cp .env.example .env
# укажите RTSP_SOURCES, например:
# RTSP_SOURCES=cam1|rtsp://user:pass@ip:554/stream1
docker compose up --build
```

- API: http://localhost:8000
- Дашборд: http://localhost:3000
- Live MJPEG: http://localhost:8000/stream/<имя_камеры>

### Повторные подключения к RTSP
- Бэкенд автоматически переподключается к камере, если поток не открылся с первой попытки или связь прервалась.
- Интервал между попытками настраивается переменной `RTSP_RECONNECT_DELAY` (по умолчанию 5 секунд).
- Количество допустимых подряд неудачных чтений кадра — `RTSP_MAX_FAILED_READS` (по умолчанию 25), после чего происходит переподключение.

### Производительность и приватность
- Для ускорения инференса визуализация поверх кадра теперь выключена по умолчанию (`VISUALIZE=false`). Если нужен поток с разметкой, явно включите переменную окружения `VISUALIZE=true`.
- Размытие лиц при сохранении снимков отключено по умолчанию (`FACE_BLUR=false`). Включайте `FACE_BLUR=true`, когда требуется дополнительная защита персональных данных.
- Хранение снимков ограничено, события в БД.
- `FACE_SAMPLE_UNVERIFIED_RETENTION_DAYS` задаёт количество дней хранения неразмеченных карточек лиц (по умолчанию 7). После истечения срока или при переводе в статусы `client`/`discarded` карточки и связанные файлы очищаются автоматически.
- Вся терминология и UI — на русском.

### Тонкая настройка воркеров
- `YOLO_DET_MODEL` и `YOLO_POSE_MODEL` — задают пути до весов YOLO (можно указать облегчённые/квантизованные варианты).
- `YOLO_FACE_MODEL` — путь до весов детектора лиц (по умолчанию `weights/yolo11n.pt`). Файл можно получить двумя способами: (1) скачать базовые веса Ultralytics, сервис попробует найти их на GitHub/HuggingFace автоматически; (2) обучить собственную модель на датасете WIDERFace командой `python -m backend.utils.train_face_detector` (подробнее ниже) и указать путь к полученному файлу вручную.
- `YOLO_FACE_CONF` — порог уверенности детектора лиц (по умолчанию 0.35). Увеличивайте значение для уменьшения ложных срабатываний, уменьшайте — если лица часто пропускаются.
- `YOLO_DEVICE` — устройство для инференса (`auto`, `cpu`, `cuda:0`, `mps` и т.д.). При значении `auto` сервис сам выберет доступный GPU (CUDA/MPS), иначе вернётся на CPU. Если вы хотите принудительно закрепить конкретную карту, укажите её явно (`YOLO_DEVICE=cuda:0`) и запустите контейнер с доступом к GPU.
- `YOLO_IMAGE_SIZE` — размер входного изображения для обоих проходов YOLO (по умолчанию 640).
- `INGEST_FPS_SKIP` — обрабатывать каждый N-й кадр (по умолчанию 2, то есть половина кадров).
- `INGEST_FLUSH_TIMEOUT` — время (в секундах), в течение которого воркер сбрасывает устаревшие кадры, чтобы взять самый свежий (по умолчанию 0.2).
- `PHONE_SCORE_SMOOTHING` — сколько последних кадров участвует в сглаживании уверенности (по умолчанию 5), помогает не терять короткие детекции.
- `POSE_ONLY_SCORE_THRESHOLD`, `POSE_ONLY_HEAD_RATIO`, `POSE_WRISTS_DIST_RATIO`, `POSE_TILT_THRESHOLD` — параметры позовой эвристики для случаев, когда телефон не детектится напрямую (руки возле головы, запястья близко друг к другу, наклон головы).

### Обучение собственного детектора лиц
- Для точного распознавания лиц можно дообучить модель Ultralytics YOLO11n на датасете WIDERFace: `python -m backend.utils.train_face_detector --device cuda:0 --epochs 50` (запускайте в контейнере backend).
- Скрипт автоматически скачает и конвертирует датасет в формат YOLO, запустит обучение и положит лучшие веса в `backend/weights/yolo11n-face.pt`. При повторных запусках используйте флаг `--skip-download`, чтобы не скачивать архивы заново; затем обновите `YOLO_FACE_MODEL`, если хотите использовать дообученный файл вместо базового `yolo11n.pt`.
- После обучения убедитесь, что переменная `YOLO_FACE_MODEL` в `.env` указывает на полученный файл (по умолчанию путь совпадает).

### Распознавание сотрудников по лицу
- Для построения эмбеддингов используется `face_recognition` (dlib, scikit-learn). Сборочные зависимости (`cmake`, `libopenblas-dev`, `liblapack-dev`, `libx11-dev` и др.) устанавливаются внутри backend-образа автоматически. При запуске вне контейнера убедитесь, что эти пакеты доступны в системе, иначе сборка `dlib` завершится ошибкой.
- Добавлены настройки `FACE_RECOGNITION_MODEL` (по умолчанию `small`), `FACE_RECOGNITION_THRESHOLD` (евклидово расстояние между эмбеддингами, по умолчанию `0.6`) и `FACE_RECOGNITION_PRESENCE_COOLDOWN` (секунды между событиями `EMPLOYEE_PRESENT` для одного сотрудника). Изменяйте их в `.env`, чтобы настроить чувствительность и частоту фиксации присутствия.
- Подготовьте для каждого сотрудника несколько чётких снимков лица (желательно фронтальных, размером не менее 256×256). Загрузите их через интерфейс /api или вручную добавьте в `static/snaps`, затем присвойте сотруднику эндпоинтом `POST /face-samples/{id}/assign`. После присвоения сервис автоматически посчитает эмбеддинг и сохранит его в базе.
- В случае отсутствующих или устаревших эмбеддингов запустите `python -m backend.utils.rebuild_face_embeddings` (в контейнере `docker compose exec backend python -m backend.utils.rebuild_face_embeddings`). Скрипт заполнит пропуски, обновит модельные версии и уведомит потоковую обработку о новых данных.
- Ингест-воркеры кэшируют эмбеддинги сотрудников в памяти. После добавления/удаления эталонных снимков кэш обновляется автоматически; тип события `EMPLOYEE_PRESENT` фиксирует присутствие сотрудника и дополняет activity-события полями `employeeId`, `employeeName`, `faceDistance`.

### Мониторинг выполнения (`GET /runtime`)
- Эндпоинт возвращает сводную информацию по доступным воркерам: раздел `summary` содержит количество поднятых потоков, число активно работающих потоков, усреднённый FPS, максимальный аптайм и время самого свежего кадра.
- В блоке `workers` теперь доступны поля `started_at`, `last_frame_at`, `uptime_seconds` и `fps`, которые показывают когда поток был запущен, когда был обработан последний кадр, сколько секунд воркер работает непрерывно и фактическую частоту обработки по скользящему окну.
- Интерфейс дашборда использует эти метрики: можно быстро понять, что поток стал отставать (падает FPS или давно не было кадров) и сколько времени конкретный воркер остаётся активным.

Практические советы:
- Если поток перегружает CPU, увеличьте `INGEST_FPS_SKIP` до 3–4 или укажите облегчённые веса (например, INT8) в `YOLO_DET_MODEL`/`YOLO_POSE_MODEL`.
- Чтобы убедиться, что события действительно распознаются, откройте `http://localhost:8000/stream/<имя_камеры>` — при детекции появится подпись `PHONE_USAGE`.
- Логи контейнера `backend` подскажут, если RTSP-поток недоступен или часто обрывается (`docker logs backend`).
